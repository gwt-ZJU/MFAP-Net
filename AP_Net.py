"""
融合模型
"""
import numpy as np
import torch
from torch import flatten, nn
from torch.nn import init
from torch.nn.modules.activation import ReLU
from torch.nn.modules.batchnorm import BatchNorm2d
from torch.nn import functional as F
from GRU import *
from SelfAttention import ScaledDotProductAttention as Attention
from einops.layers.torch import Rearrange, Reduce
import math
import warnings
from Res_FAMLP import Res_MLP
from RSE_Branch import RSE
from Resnet import resnet50 as Resnet
from HFAF import HFAF
from Res_GCMFE import GCMFE
from functools import partial
import numpy as np

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)



class AP_Net(nn.Module):
    def __init__(self,fuse_flag = 'HFAF',time_len = 4,Guide = 'st'):
        """Dense version of GAT."""
        super(AP_Net, self).__init__()

        self.cnn = GCMFE(include_top=False)
        self.time_seg = RSE(time_len=time_len,CGAF_Flag=True)
        self.static_encoder = Res_MLP()
        self.fuse_model = HFAF(fuse_flag=fuse_flag,time_len=time_len,Guide=Guide)
        self.static_encoder.register_forward_hook(self.get_static_output)

        self.apply(self._init_weights)

    def get_static_output(self, module, input, output):
        self.static_output = output

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, st_feature,seg_feature,static_feature):
        cnn_feature = self.cnn(st_feature)
        seg_feature = self.time_seg(seg_feature)
        static_feature = self.static_encoder(static_feature)
        out = self.fuse_model(cnn_feature,seg_feature,static_feature)

        # out = F.softmax(out, dim=1)
        return out







if __name__ == '__main__':

    """
    """
    time_len = 5
    st_feature = torch.rand(4,3, 256, 256).cuda()
    seg_feature = torch.rand(4,time_len, 47).cuda()
    static_feature = torch.rand(4, 37).cuda()

    model_1 = AP_Net(time_len = time_len,fuse_flag='seg_static').cuda()
    out = model_1(st_feature,seg_feature,static_feature)
    pass
