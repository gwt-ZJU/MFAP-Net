"""
这部分是卫星图像处理的部分
"""

import numpy as np
import torch
from torch import flatten, nn
from torch.nn import init
from torch.nn.modules.activation import ReLU
from torch.nn.modules.batchnorm import BatchNorm2d
from torch.nn import functional as F
from einops import rearrange
from einops.layers.torch import Rearrange, Reduce
from pool import *
import math
import warnings
from functools import partial
import numpy as np

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    r"""
    Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.
    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        # >>> w = torch.empty(3, 5)
        # >>> nn.init.trunc_normal_(w)
    """
    return _no_grad_trunc_normal_(tensor, mean, std, a, b)

class SELayer(nn.Module):
    def __init__(self, channel, reduction=16):
        super(SELayer, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
            nn.Linear(channel, channel // reduction, bias=False),
            nn.GELU(),
            nn.Linear(channel // reduction, channel, bias=False),
            nn.Sigmoid()
        )
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y.expand_as(x)


class DepthwiseSeparableConvolution(nn.Module):
    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1):
        super().__init__()
        self.depthwise_conv = nn.Conv2d(
            in_channels=in_ch,
            out_channels=in_ch,
            kernel_size=kernel_size,
            stride=stride,
            padding=padding,
            groups=in_ch
        )
        self.pointwise_conv = nn.Conv2d(
            in_channels=in_ch,
            out_channels=out_ch,
            kernel_size=1,
            stride=1,
            padding=0,
            groups=1
        )

    def forward(self, x):
        out = self.depthwise_conv(x)
        out = self.pointwise_conv(out)
        return out

class FeedForward(nn.Module):
    def __init__(self,dim,hidden_dim,dropout=0.):
        super().__init__()
        self.net=nn.Sequential(
            #由此可以看出 FeedForward 的输入和输出维度是一致的
            nn.Linear(dim,hidden_dim),
            #激活函数
            nn.GELU(),
            #防止过拟合
            nn.Dropout(dropout),
            #重复上述过程
            nn.Linear(hidden_dim,dim),
            nn.Dropout(dropout)
        )
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self,x):
        x=self.net(x)
        return x

class Mixer(nn.Module):
    def __init__(self,feature_size = 8,patch_size=1,inchannel=512,depth=1,token_dim=512,dropout=0.5):
        super(Mixer, self).__init__()
        self.num_patches = (feature_size // patch_size) ** 2
        self.token_dim = self.num_patches *2
        self.channel_dim = inchannel * 4

        self.to_embedding = nn.Sequential(
            nn.Conv2d(in_channels=inchannel, out_channels=inchannel, kernel_size=patch_size, stride=patch_size),
            Rearrange('b c h w -> b (h w) c')
            )

        self.layer_normal = nn.LayerNorm(inchannel)


        self.token_mixer = nn.Sequential(
            nn.LayerNorm(inchannel),
            Rearrange('b n d -> b d n'),
            FeedForward(self.num_patches, self.token_dim, dropout),
            Rearrange('b d n -> b n d')
        )

        self.channel_mixer = nn.Sequential(
            nn.LayerNorm(inchannel),
            FeedForward(inchannel, self.channel_dim, dropout)
        )

        self.size_reshape = nn.Sequential(
            Rearrange('b (h w) c -> b c h w',h=feature_size // patch_size,w=feature_size // patch_size),
            # nn.Conv2d(in_channels=inchannel, out_channels=inchannel, kernel_size=patch_size, stride=patch_size),
            )

    def forward(self,x):
        _,_,H,W =x.shape
        x = self.to_embedding(x)

        """
        分别是空间和通道
        """
        spatial_feature = self.token_mixer(x)
        # channel_feature = self.channel_mixer(x)

        x = spatial_feature  + x

        x = self.layer_normal(x)
        x = self.size_reshape(x)
        x = F.interpolate(x, size=(H, W), mode='bilinear', align_corners=True)
        return x

class LocalFeatureExtractionUnit(nn.Module):
    def __init__(self, indim,dim, act=True):
        super(LocalFeatureExtractionUnit, self).__init__()
        """
        这个是Block的第一部分内容，叫做 局部特征提取单元
        :param dim:维度
        :param act: 是否使用GELU函数和BN
        """
        self.act = act
        self.conv_3x3 = nn.Conv2d(in_channels=indim,out_channels=indim,kernel_size=3,padding=1)
        self.conv_1x1 = nn.Conv2d(in_channels=indim,out_channels=dim,kernel_size=1,padding=0)
        if self.act:
            self.actation = nn.Sequential(
                nn.BatchNorm2d(dim),
                nn.GELU(),
                nn.Dropout(0.3)
            )
        self.RelU = nn.GELU()
        self.SE_layer = SELayer(channel=indim)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self,x):
        if self.act:
            out = self.RelU(self.conv_3x3(x))
            # out = self.SE_layer(out) + x
            out = self.actation(self.conv_1x1(out))
            return out
        else:
            out = self.RelU(self.conv_3x3(x)) + x
            out = self.conv_1x1(out)
            return out

class Attention_embed(nn.Module):
    def __init__(self,dim,kernel_size):
        super(Attention_embed, self).__init__()
        factor = 4 #这个是因子

        self.spatial_embed = nn.Sequential(
            nn.Conv2d(2*dim,2*dim//factor,1,bias=False),
            nn.BatchNorm2d(2*dim//factor),
            nn.ReLU(),
            nn.Conv2d(2*dim//factor,kernel_size*kernel_size*dim,1)
        )

        self.con1x1 = nn.Conv2d(in_channels=2 * dim, out_channels=dim, kernel_size=1, bias=False)
        self.channel_embed = SELayer(channel=dim)
        self.bn = nn.BatchNorm2d(dim*2//factor)
        self.relu = nn.GELU()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self,x):
        bs, c, h, w = x.shape
        """
        空间
        """
        spatial_feature = self.spatial_embed(x)
        spatial_feature = spatial_feature.reshape(bs,int(c/2),3*3,h,w)
        spatial_feature = spatial_feature.mean(2,keepdim=False)
        """
        通道
        """
        # channel_feature = self.con1x1(x)
        # channel_feature = self.channel_embed(channel_feature) + channel_feature
        """
        空间通道融合
        """
        att = spatial_feature
        return att

class SpatialAttentionModule(nn.Module):
    def __init__(self):
        super(SpatialAttentionModule, self).__init__()
        self.conv2d = nn.Conv2d(in_channels=2, out_channels=1, kernel_size=3, stride=1, padding=1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = torch.mean(x, dim=1, keepdim=True)
        maxout, _ = torch.max(x, dim=1, keepdim=True)
        out = torch.cat([avgout, maxout], dim=1)
        out = self.sigmoid(self.conv2d(out))
        out = out * x + x
        return out

# class MFAFN(nn.Module):
#     """
#     起名叫做:特征聚合前馈网络
#     定义：
#     1x1的卷积：dim==>out_dim
#     """
#     def __init__(self,in_channel,dim_ratio=4):
#         super(MFAFN, self).__init__()
#         out_dim = in_channel * 2
#         self.conv1x1_gelu_bn = nn.Sequential(
#             nn.Conv2d(in_channels=in_channel,out_channels=out_dim,kernel_size=1,stride=1),
#             nn.GELU(),
#             nn.BatchNorm2d(out_dim)
#         )
#
#         # self.conv1x1 = nn.Conv2d(in_channels=out_dim, out_channels=out_dim//2, kernel_size=1, padding=0)
#         # self.conv3x3 = nn.Conv2d(in_channels=out_dim,out_channels=out_dim//4,kernel_size=3,padding=1)
#         # self.conv5x5 = nn.Conv2d(in_channels=out_dim,out_channels=out_dim//4,kernel_size=5,padding=2)
#
#         self.conv1x1 = DepthwiseSeparableConvolution(in_ch=out_dim,out_ch=out_dim//2,kernel_size=1,stride=1,padding=0)
#         self.conv3x3 = DepthwiseSeparableConvolution(in_ch=out_dim,out_ch=out_dim//4,kernel_size=3,stride=1,padding=1)
#         self.conv5x5 = DepthwiseSeparableConvolution(in_ch=out_dim,out_ch=out_dim//4,kernel_size=5,stride=1,padding=2)
#
#         self.Spatial_embed = SpatialAttentionModule()
#
#         self.act = nn.Sequential(
#             nn.GELU(),
#             nn.BatchNorm2d(out_dim)
#         )
#
#         # self.dw_conv_a = DepthwiseSeparableConvolution(in_ch=out_dim,out_ch=out_dim//2)
#
#         self.conv1x1_bn = nn.Sequential(
#             nn.Conv2d(in_channels=out_dim,out_channels=out_dim,kernel_size=1,stride=1),
#             nn.BatchNorm2d(out_dim)
#         )
#         self.apply(self._init_weights)
#
#     def _init_weights(self, m):
#         if isinstance(m, nn.Linear):
#             trunc_normal_(m.weight, std=.02)
#             if isinstance(m, nn.Linear) and m.bias is not None:
#                 nn.init.constant_(m.bias, 0)
#         elif isinstance(m, nn.LayerNorm):
#             nn.init.constant_(m.bias, 0)
#             nn.init.constant_(m.weight, 1.0)
#         elif isinstance(m, nn.Conv2d):
#             fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
#             fan_out //= m.groups
#             m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
#             if m.bias is not None:
#                 m.bias.data.zero_()
#
#
#     def forward(self,x):
#         """
#         1.首次经过1x1卷积，修改通道数
#         2.不同尺度卷积
#         3.cat之后修改通道，并且进行残差
#         :param x:
#         :return:
#         """
#         identity = self.conv1x1_gelu_bn(x)
#
#         """
#         正常的三种尺度
#         """
#         out_1x1 = self.conv1x1(identity)
#         out_3x3 = self.conv3x3(identity)
#         out_5x5 = self.conv5x5(identity)
#         """
#         一个全局的空间分支
#         """
#         spatial_feature = self.Spatial_embed(identity)
#         """
#         后面的相加 经过1x1卷积
#         """
#         cat_feature = torch.cat([out_1x1,out_3x3,out_5x5],dim=1)
#         cat_feature = cat_feature + spatial_feature
#         cat_feature = cat_feature
#
#         out = self.act(cat_feature)
#         out = self.conv1x1_bn(out)
#         return out+identity


class MFAFN(nn.Module): #多尺度特征聚合Cell
    def __init__(self,in_channel,dim_ratio=4):
        super(MFAFN,self).__init__()
        """
        定义不同尺度的卷积核
        """
        # self.conv1x1_conv = nn.Conv2d(in_channel, in_channel*2, kernel_size=1, stride=1, padding=0, bias=False)
        self.conv1x1_conv = DepthwiseSeparableConvolution(in_ch=in_channel, out_ch=in_channel*2, kernel_size=1, stride=1, padding=0)
        self.bn1 = nn.BatchNorm2d(in_channel*2)

        self.conv1x1_a =  DepthwiseSeparableConvolution(in_ch=in_channel*2, out_ch=in_channel//2, kernel_size=1, stride=1, padding=0)
        self.conv3x3_a = DepthwiseSeparableConvolution(in_ch=in_channel*2, out_ch=in_channel//dim_ratio, kernel_size=3, stride=1, padding=1)
        self.conv5x5_a = DepthwiseSeparableConvolution(in_ch=in_channel*2, out_ch=in_channel//dim_ratio, kernel_size=5, stride=1, padding=2)
        self.conv1x1_b = DepthwiseSeparableConvolution(in_ch=in_channel//dim_ratio, out_ch=in_channel, kernel_size=1, stride=1, padding=0)
        self.conv3x3_b = DepthwiseSeparableConvolution(in_ch=in_channel//2, out_ch=in_channel//2, kernel_size=3, stride=1, padding=1)
        self.conv5x5_b = DepthwiseSeparableConvolution(in_ch=in_channel//2, out_ch=in_channel//2, kernel_size=5, stride=1, padding=2)


        # self.conv1x1_a = nn.Conv2d(in_channel*2, in_channel//2, kernel_size=1, stride=1, padding=0, bias=False)
        # self.conv3x3_a = nn.Conv2d(in_channel*2, in_channel//dim_ratio, kernel_size=3, stride=1, padding=1, bias=False)
        # self.conv5x5_a = nn.Conv2d(in_channel*2, in_channel//dim_ratio, kernel_size=5, stride=1, padding=2, bias=False)
        # self.conv1x1_b = nn.Conv2d(in_channel//dim_ratio, in_channel, kernel_size=1, stride=1, padding=0, bias=False)
        # self.conv3x3_b = nn.Conv2d(in_channel//2, in_channel//2, kernel_size=3, stride=1, padding=1, bias=False)
        # self.conv5x5_b = nn.Conv2d(in_channel//2, in_channel//2, kernel_size=5, stride=1, padding=2, bias=False)
        self.conv_fuse_a = nn.Conv2d(int(in_channel*1), in_channel//dim_ratio, kernel_size=1, stride=1, padding=0, bias=False)
        self.GLEU = nn.GELU()
        self.conv1x1_c = nn.Conv2d(in_channel*2, in_channel*2, kernel_size=1, stride=1, padding=0, bias=False)
        self.BN = nn.BatchNorm2d(in_channel*2)
        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)
        elif isinstance(m, nn.Conv2d):
            fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
            fan_out //= m.groups
            m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
            if m.bias is not None:
                m.bias.data.zero_()

    def forward(self, x):
        x = self.bn1(self.GLEU(self.conv1x1_conv(x)))
        identity = x
        x_1 = self.conv1x1_a(x)
        x_3 = self.conv3x3_a(x)
        x_5 = self.conv5x5_a(x)
        fuse_feature = torch.cat([x_1,x_3,x_5],dim=1)
        fuse_feature = self.conv_fuse_a(fuse_feature)
        x_3 = torch.cat([x_3,fuse_feature],dim=1)
        x_5 = torch.cat([x_5, fuse_feature], dim=1)
        x_1 = self.conv1x1_b(fuse_feature)
        x_3 = self.conv3x3_b(x_3)
        x_5 = self.conv5x5_b(x_5)
        out = torch.cat([x_1,x_3,x_5],dim=1)
        out = self.BN(self.conv1x1_c(out))+identity
        return out

class GCRT(nn.Module):
    def __init__(self,dim,feature_size,kernel_size=3):
        super(GCRT, self).__init__()
        """
        self.value_embed:对value进行编码
        self.key_multi_embed:对key进行多尺度上下文信息提取，作为图像的全域表达
        """
        self.dim = dim
        self.feature_size = feature_size

        self.value_embed = nn.Sequential(
            nn.Conv2d(in_channels=self.dim, out_channels=self.dim, kernel_size=1),
            nn.BatchNorm2d(self.dim)
        )

        self.key_embed = self.key_embed=nn.Sequential(
                        nn.Conv2d(dim,dim,kernel_size=kernel_size,padding=kernel_size//2,groups=4,bias=False),
                        nn.BatchNorm2d(dim),
                        nn.ReLU()
                    )

        self.att_embed = Attention_embed(dim=self.dim,kernel_size=kernel_size)
        self.mixer_embed = Mixer(feature_size=self.feature_size,patch_size=4,inchannel=self.dim)

    def forward(self,x):
        bs,c,h,w = x.shape
        """
        0:进行通道和空间的特征提取
        1.提取全域不同尺度上下文信息获得key
        2.得到value编码
        3.Key与Query在channel维度上进行拼接进行拼接
        4.求不同尺度的注意力矩阵的均值
        5.计算key_2的矩阵，是用sotgmax之后的注意力矩阵*value获取
        6.进行全域特征的二次融合
        """
        #0
        # key_embed = self.mixer_embed(x)
        #1
        key_1 = self.key_embed(x)
        #2
        value = self.value_embed(x).view(bs, c, -1)
        #3
        y = torch.cat([key_1,x],dim=1)
        #4
        att = self.att_embed(y).view(bs,c,-1)
        #5
        key_2 = F.softmax(att,dim=-1) * value
        #6
        key_2 = key_2.view(bs,c,h,w)
        return  key_2+key_1

class Resnet_Bottleneck(nn.Module):
    def __init__(self,in_channel,groups=1, width_per_group=64,stride=1,):
        super(Resnet_Bottleneck, self).__init__()
        width = int(in_channel*2 * (width_per_group / 64.)) * groups
        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,
                               kernel_size=1, stride=1, bias=False)  # squeeze channels
        self.bn1 = nn.BatchNorm2d(width)
        # -----------------------------------------
        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,
                               kernel_size=3, stride=stride, bias=False, padding=1)
        self.conv3 = nn.Conv2d(in_channels=width, out_channels=in_channel,
                               kernel_size=1, stride=1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.bn2 = nn.BatchNorm2d(width)
    def forward(self,x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        return out

class LEU(nn.Module):
    def __init__(self,in_channel,out_channel,groups=1, width_per_group=64,stride=1,):
        super(LEU, self).__init__()
        width = int(in_channel*2 * (width_per_group / 64.)) * groups
        self.conv1 = nn.Conv2d(in_channels=in_channel, out_channels=width,
                               kernel_size=1, stride=1, bias=False)  # squeeze channels
        self.bn1 = nn.BatchNorm2d(width)
        # -----------------------------------------
        self.conv2 = nn.Conv2d(in_channels=width, out_channels=width, groups=groups,
                               kernel_size=3, stride=stride, bias=False, padding=1)
        self.conv3 = nn.Conv2d(in_channels=width, out_channels=out_channel,
                               kernel_size=1, stride=1, bias=False)
        self.relu = nn.ReLU(inplace=True)
        self.bn2 = nn.BatchNorm2d(width)
    def forward(self,x):
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)
        out = self.conv3(out)
        return out


class CWT_Bottleneck(nn.Module):
    expansion = 2
    def __init__(self,indim,dim,stride=1,downsample=None,use_LEU=True,use_GCT=True,use_FAFN=True,embed_dims=None,num_heads=None,mlp_ratios=None,qkv_bias=None,
                    norm_layer=None,sr_ratios=None,drop_rate=None,drop_path_rate=None,feature_size = 256):
        super(CWT_Bottleneck, self).__init__()
        self.use_LEU = use_LEU
        self.use_GCT = use_GCT
        self.use_FAFN = use_FAFN

        self.feature_size = feature_size
        self.dim = dim
        self.downsample = downsample
        self.stride = stride
        self.LocalFeatureExtractionUnit = LocalFeatureExtractionUnit(indim=indim,dim=self.dim)
        if self.use_LEU:
            # self.LocalFeatureExtractionUnit = LocalFeatureExtractionUnit(indim=indim, dim=self.dim)
            self.LocalFeatureExtractionUnit = LEU(in_channel=indim,out_channel=dim)
        if self.use_LEU is not True: #FEU 特征抽取单元
            self.FEU = nn.Sequential(
                nn.Conv2d(in_channels=indim,out_channels=dim,kernel_size=1,padding=0),
                nn.GELU(),
                nn.BatchNorm2d(dim)
            )
        if self.use_GCT:
            self.GCRT = GCRT(dim=self.dim,feature_size=self.feature_size)
        if self.use_GCT is not True:
            self.Resnet_Bottleneck = Resnet_Bottleneck(in_channel=self.dim)
            # VIT_Block = Block(dim=embed_dims, num_heads=num_heads, mlp_ratio=mlp_ratios, qkv_bias=qkv_bias,
            #         qk_scale=False,
            #         drop=drop_rate, attn_drop=0., drop_path=drop_path_rate, norm_layer=norm_layer,
            #         sr_ratio=sr_ratios)
            # self.Transformer_VIT_Branch = Transformer_VIT_Branch(embed_dim=embed_dims,block=VIT_Block)
        if self.use_FAFN:
            self.MFAFN = MFAFN(in_channel=self.dim)
        if self.use_FAFN is not True:
            self.FFN = nn.Sequential(
                nn.Conv2d(in_channels=self.dim, out_channels=self.dim*2, kernel_size=1, padding=0),
                nn.GELU()
            )
        self.layer_norm_1 = nn.LayerNorm(self.dim)
        self.layer_norm_2 = nn.LayerNorm(self.dim)




    def forward(self,x):
        """
        0.下采样特征准备
        1.局部特征抽取，在LocalFeatureExtractionUnit里面进行了残差
        2.特征进行翻转，需要进行layernorm
        3.全域上下文提取（需要加上残差）
        4.特征进行翻转，需要进行layernorm
        5.特征聚合前馈网络
        :param x:
        :return:
        """
        """
        排列组合
        1. use_LEU=True   use_GCT=True       use_FAFN=True   SFRAN (FAFN+GCT+VIT)
        2. use_LEU=True   use_GCT=True       use_FAFN=False  GCT+LEU
        3. use_LEU=False  use_GCT=True       use_FAFN=True   GCT+FAFN
        4. use_LEU=True   use_GCT=False(VIT) use_FAFN=True   FAFN+LEU+VIT
        """
        #0
        identity = x
        if self.downsample is not None:
            identity = self.downsample(x)
        #1
        if self.use_LEU:
            local_feature = self.LocalFeatureExtractionUnit(x)
        else:
            local_feature = self.FEU(x)
        #2
        local_feature_norm = local_feature
        #3
        if self.use_GCT:
            Global_feature = self.GCRT(local_feature_norm)
            Global_feature = Global_feature + local_feature
        else:
            Global_feature = self.Resnet_Bottleneck(local_feature_norm)
            # Global_feature = self.Transformer_VIT_Branch(local_feature_norm)
            Global_feature = Global_feature + local_feature

        #4
        Global_feature_norm = Global_feature
        #5
        if self.use_FAFN:
            Agg_feature = self.MFAFN(Global_feature_norm)
        else:
            Agg_feature = self.FFN(Global_feature_norm)
            # Agg_feature = Global_feature_norm

        return Agg_feature + identity


class PatchAggregation(nn.Module):
    """down sample the feature resolution, build with conv 2x2 stride 2
    """

    def __init__(self, in_channel, out_channel, kernel_size=2, stride_size=2):
        super(PatchAggregation, self).__init__()
        self.patch_aggregation = nn.Conv2d(
            in_channels=in_channel,
            out_channels=out_channel,
            kernel_size=kernel_size,
            stride=stride_size
        )
        # self.patch_aggregation = MixPool2d(kernel_size=2)

    def forward(self, x):
        x = self.patch_aggregation(x)
        return x


class CWT_Net(nn.Module):
    def __init__(self, block = CWT_Bottleneck, layers=[2,2,2,2], num_classes=2,use_LEU=True,use_GCT=True,use_FAFN=False,
                 embed_dims=[64, 128, 320,512], num_heads=[1, 2, 5,8], mlp_ratios=[4, 4, 4,4],
                 qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2,2,2,2], sr_ratios=[8, 4, 2,1],
                 drop_rate=0.0, drop_path_rate=0.1
                 ):
        self.inplanes = 32
        super(CWT_Net, self).__init__()
        self.use_LEU = use_LEU
        self.use_GCT = use_GCT
        self.use_FAFN = use_FAFN

        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=5, stride=2, padding=2,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(self.inplanes)
        self.relu = nn.GELU()
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        # self.layer1 = self._make_layer(block, 64, layers[0], stride=1)
        # self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        # self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        # self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        if self.use_GCT:
            self.layer1 = self._make_layer(block, 32, layers[0], stride=1,feature_size=64)
            self.layer2 = self._make_layer(block, 64, layers[1], stride=2,feature_size=32)
            self.layer3 = self._make_layer(block, 128, layers[2], stride=2,feature_size=16)
            self.layer4 = self._make_layer(block, 256, layers[3], stride=2,feature_size=8)

        if self.use_GCT is not True:
            self.layer1 = self._make_layer(block, 32, depths[0], stride=1,embed_dims= embed_dims[0],num_heads =num_heads[0],mlp_ratios =mlp_ratios[0],qkv_bias=qkv_bias,
                                           norm_layer=norm_layer,sr_ratios=sr_ratios[0],drop_rate=drop_rate,drop_path_rate=drop_path_rate)
            self.layer2 = self._make_layer(block, 64, depths[1], stride=2,embed_dims= embed_dims[1],num_heads =num_heads[1],mlp_ratios =mlp_ratios[1],qkv_bias=qkv_bias,
                                           norm_layer=norm_layer,sr_ratios=sr_ratios[1],drop_rate=drop_rate,drop_path_rate=drop_path_rate)
            self.layer3 = self._make_layer(block, 128, depths[2], stride=2,embed_dims= embed_dims[2],num_heads =num_heads[2],mlp_ratios =mlp_ratios[2],qkv_bias=qkv_bias,
                                           norm_layer=norm_layer,sr_ratios=sr_ratios[2],drop_rate=drop_rate,drop_path_rate=drop_path_rate)
            self.layer4 = self._make_layer(block, 256, depths[3], stride=2, embed_dims=embed_dims[3],
                                           num_heads=num_heads[3], mlp_ratios=mlp_ratios[3], qkv_bias=qkv_bias,
                                           norm_layer=norm_layer, sr_ratios=sr_ratios[3], drop_rate=drop_rate,
                                           drop_path_rate=drop_path_rate)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))

        self.pool1 = PatchAggregation(in_channel=64, out_channel=64)
        self.pool2 = PatchAggregation(in_channel=128,out_channel=128)
        self.pool3 = PatchAggregation(in_channel=256, out_channel=256)
        self.pool4 = PatchAggregation(in_channel=512, out_channel=512)
        self.out_fc = nn.Linear(512, 2)


        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def _make_layer(self, block, planes, blocks, stride=1,embed_dims=None,num_heads=None,mlp_ratios=None,qkv_bias=None,
                    norm_layer=None,sr_ratios=None,drop_rate=None,drop_path_rate=None,feature_size=256):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=1, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample,use_LEU=self.use_LEU,use_GCT=self.use_GCT,use_FAFN=self.use_FAFN,embed_dims=embed_dims,num_heads=num_heads,
                            mlp_ratios=mlp_ratios, qkv_bias=qkv_bias,norm_layer=norm_layer,sr_ratios=sr_ratios,drop_rate=drop_rate,drop_path_rate=drop_path_rate,feature_size=feature_size))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes,use_LEU=self.use_LEU,use_GCT=self.use_GCT,use_FAFN=self.use_FAFN,embed_dims=embed_dims,num_heads=num_heads,
                            mlp_ratios=mlp_ratios, qkv_bias=qkv_bias,norm_layer=norm_layer,sr_ratios=sr_ratios,drop_rate=drop_rate,drop_path_rate=drop_path_rate,feature_size=feature_size))
        return nn.Sequential(*layers)

    def forward(self, x):
        #stem
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        s1 = self.pool1(x)
        # s1 = x

        s2 = self.layer2(s1)
        s2 = self.pool2(s2)

        s3 = self.layer3(s2)
        s3 = self.pool3(s3)

        s4 = self.layer4(s3)
        x = self.avgpool(s4)
        x = torch.flatten(x, 1)
        out = self.out_fc(x)
        # s4 = self.pool4(s4)


        return out

if __name__ == '__main__':
    layers = [2,2,2,2]
    x = torch.rand(16,3,256,256)
    cell = CWT_Net()
    out = cell(x)
    pass
